{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git  -U \n!pip install -q git+https://github.com/huggingface/accelerate.git  -U \n!pip install -q bitsandbytes \n!pip install -q git+https://github.com/huggingface/peft.git  -U \n!pip install -q langchain\n!pip install -q sentence-transformers\n!pip install -q faiss-gpu\n!pip install requests","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:07:36.185649Z","iopub.execute_input":"2024-02-20T22:07:36.186397Z","iopub.status.idle":"2024-02-20T22:10:30.016571Z","shell.execute_reply.started":"2024-02-20T22:07:36.186340Z","shell.execute_reply":"2024-02-20T22:10:30.015605Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.11 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Extracting information from webpages","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport time\n\n\ndef get_search_urls(query, num_results=5):\n    \"\"\"\n    Retrieves URLs from a search engine for a given query.\n\n    Args:\n        query (str): The search query.\n        search_engine (str): The search engine name (e.g., \"google\", \"bing\").\n        num_results (int): The desired number of results (default: 20).\n\n    Returns:\n        list: A list of top URLs from the search.\n\n    Raises:\n        ValueError: If an invalid search engine is provided.\n    \"\"\"\n\n    # Construct Google search URL with proper encoding\n    for i in query:\n        if i==\" \":\n            query+=\"+\"\n        else:\n            query+=i\n    print(query)\n    query = \"https://www.google.dz/search?q=\"+query\n    page = requests.get(query)\n    soup = BeautifulSoup(page.content,features=\"html.parser\")\n    url_list=[]\n    count=0\n    for link in soup.find_all(\"a\",href=re.compile(\"(?<=/url\\?q=)(htt.*://.*)\")):\n        if count == 0:\n            count+=1\n        else:\n            url = re.split(\":(?=http)\",link[\"href\"].replace(\"/url?q=\",\"\"))[0]\n            url = re.sub(r\"&sa.*\", \"\", url)\n            url_list.append(url)\n    return url_list[:num_results]","metadata":{"_uuid":"894e130e-b147-4eb8-b77d-5520251372da","_cell_guid":"f794b751-8af7-4e32-89d2-73911271ad55","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-20T22:10:30.018970Z","iopub.execute_input":"2024-02-20T22:10:30.019720Z","iopub.status.idle":"2024-02-20T22:10:30.259351Z","shell.execute_reply.started":"2024-02-20T22:10:30.019680Z","shell.execute_reply":"2024-02-20T22:10:30.258608Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def scrape_url(url):\n    \"\"\"\n    Scrapes data from a given URL.\n\n    Args:\n        url (str): The URL to scrape.\n\n    Returns:\n        dict: A dictionary containing scraped data (format depends on website).\n    \"\"\"\n    try:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        text_list = []\n        for i in [\"p\",\"code\",\"answers\",\"discussion_bucket\",\"content_main\"]:\n            for p in soup.find_all(i):  # Replace \"p\" with relevant tags\n                text_list.append(p.text.strip())  # Remove leading/trailing whitespace\n\n        # Combine and clean text\n        all_text = \"\\n\".join(text_list)\n        cleaned_text = re.sub(r\"<.*?>\", \"\", all_text)  # Remove HTML tags with regex\n        count = 0\n        final_text = \"\"\n        for i in range(0,len(cleaned_text)):\n            if cleaned_text[i]==\"\\n\" and cleaned_text[i]==\"\\n\":\n                count+=1\n                if count==200:\n                    break\n            final_text+=cleaned_text[i]\n        return final_text\n    except:\n        return \"\"","metadata":{"_uuid":"894e130e-b147-4eb8-b77d-5520251372da","_cell_guid":"f794b751-8af7-4e32-89d2-73911271ad55","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-20T22:10:30.260478Z","iopub.execute_input":"2024-02-20T22:10:30.260901Z","iopub.status.idle":"2024-02-20T22:10:30.268331Z","shell.execute_reply.started":"2024-02-20T22:10:30.260876Z","shell.execute_reply":"2024-02-20T22:10:30.267493Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def WebCrawler(query):\n    # Get user input\n    begin = time.time()\n\n    # Retrieve top URLs\n    search_urls = get_search_urls(query)\n\n    txt_file_for_urls=\"/kaggle/working/URLlist.txt\"\n    with open(txt_file_for_urls, \"w+\") as file:  # Open in append mode\n        for website in search_urls:\n            try:\n                data = scrape_url(website)  # Call your function\n                # Combine and format data before saving\n                formatted_data = f\"Website: {website}\\n{data}\\n\\n\"  # Separate by website\n                file.write(formatted_data)\n            except Exception as e:\n                print(f\"Error scraping {website}: {e}\")\n    print(\"Time taken to execute: \", time.time()-begin)","metadata":{"_uuid":"894e130e-b147-4eb8-b77d-5520251372da","_cell_guid":"f794b751-8af7-4e32-89d2-73911271ad55","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-20T22:10:30.270771Z","iopub.execute_input":"2024-02-20T22:10:30.271063Z","iopub.status.idle":"2024-02-20T22:10:30.283587Z","shell.execute_reply.started":"2024-02-20T22:10:30.271005Z","shell.execute_reply":"2024-02-20T22:10:30.282776Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"get_search_urls(\"What is the best shampoo in India?\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:10:30.284546Z","iopub.execute_input":"2024-02-20T22:10:30.284794Z","iopub.status.idle":"2024-02-20T22:10:31.534995Z","shell.execute_reply.started":"2024-02-20T22:10:30.284772Z","shell.execute_reply":"2024-02-20T22:10:31.533946Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"What is the best shampoo in India?What+is+the+best+shampoo+in+India?\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['https://www.newindianexpress.com/expressdeals/other-categories/beauty-and-health/best-shampoo-for-hair-growth-in-india',\n 'https://www.newindianexpress.com/expressdeals/other-categories/beauty-and-health/best-shampoo-for-hair-growth-in-india',\n 'https://www.google.comhttps://support.google.com/websearch%3Fp%3Dfeatured_snippets%26hl%3Den-US&opi=89978449&usg=AOvVaw2M8SCMBzjmvsKhurArc7_8',\n 'https://www.vikatan.com/vikatan-deals/reviews/health-beauty/10-best-shampoos-for-hair-growth',\n 'https://www.outlookindia.com/whats-hot/10-best-shampoos-for-dry-hair-in-india-2024']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Initializing LLM","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\n# Create a quantization configuration using BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.float16,\n    bnb_4bit_use_double_quant= True,\nllm_int8_enable_fp32_cpu_offload= True)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:10:31.536137Z","iopub.execute_input":"2024-02-20T22:10:31.536484Z","iopub.status.idle":"2024-02-20T22:10:36.885413Z","shell.execute_reply.started":"2024-02-20T22:10:31.536456Z","shell.execute_reply":"2024-02-20T22:10:36.884632Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",quantization_config=quantization_config)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:10:36.886426Z","iopub.execute_input":"2024-02-20T22:10:36.886792Z","iopub.status.idle":"2024-02-20T22:11:56.575721Z","shell.execute_reply.started":"2024-02-20T22:10:36.886768Z","shell.execute_reply":"2024-02-20T22:11:56.574736Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f9be253bbe45288dad422a8de64221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b751909047aa4468989e3ca3285bf79c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53239c07e45c4403a73c23422503cdd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba015345fc7a460c81548c7fb619c633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91c014db43184acfa6998cc6961f831b"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a105fa9048a742ceb8a031ac74204c70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa1917ed20e49d9a8a6e6dc919a55c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d9e517603ca47049e136eaa0c04773e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fdf3b6c8e63458da78a91355b19b086"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c5ab64acc048a7a917493c04b5f738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf84f6c37d942bcafed3edfb4784b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"837e35f324ec4f3ab4ab93f99df21bd0"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Initializing Vector Database","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import FAISS\nimport torch\nfrom transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:11:56.576861Z","iopub.execute_input":"2024-02-20T22:11:56.577301Z","iopub.status.idle":"2024-02-20T22:12:11.892549Z","shell.execute_reply.started":"2024-02-20T22:11:56.577274Z","shell.execute_reply":"2024-02-20T22:12:11.891640Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-02-20 22:11:59.058417: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-20 22:11:59.058549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-20 22:11:59.198186: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the model name for Hugging Face embeddings\nmodel_name = 'WhereIsAI/UAE-Large-V1'\n\n# Define model kwargs for Hugging Face embeddings during initialization\nmodel_kwargs = {'device': 'cuda'}\n\n# Define encode kwargs for Hugging Face embeddings\n# Set 'normalize_embeddings' to True to compute cosine similarity\nencode_kwargs = {'normalize_embeddings': True}\n\n# Create Hugging Face embeddings instance\n# Specify the model name, model kwargs, and encode kwargs\nembeddings = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:12:11.893700Z","iopub.execute_input":"2024-02-20T22:12:11.894255Z","iopub.status.idle":"2024-02-20T22:12:20.337862Z","shell.execute_reply.started":"2024-02-20T22:12:11.894228Z","shell.execute_reply":"2024-02-20T22:12:20.336800Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/733 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9280da3cb36a4a09ba022bda41251978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"808a241d856648479633daaf22e8ffe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe9408fda5784abd9b64a8442f253153"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd455b36720f4ac28d0c6b24a18f884a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7475680d51ef47819245349e7d3598a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80625a7d69d84bab8cfb7538992af3d2"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Splitting the document","metadata":{}},{"cell_type":"code","source":"def docSplitter():\n    # Create a TextLoader instance for loading text documents\n    # Specify the path to the text file (\"/kaggle/input/evspdf/RAG.txt\" in this case)\n    loader = TextLoader(\"/kaggle/working/URLlist.txt\")\n\n    # Load documents using the TextLoader\n    documents = loader.load()\n    # Create a RecursiveCharacterTextSplitter instance for splitting text documents\n    # Set a small chunk size and overlap for demonstration purposes\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=500,\n        length_function=len,\n        is_separator_regex=False\n    )\n\n    # Split the loaded documents using the TextSplitter\n    text_docs = text_splitter.split_documents(documents)\n    \n    return text_docs","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:12:20.340649Z","iopub.execute_input":"2024-02-20T22:12:20.340945Z","iopub.status.idle":"2024-02-20T22:12:20.346699Z","shell.execute_reply.started":"2024-02-20T22:12:20.340920Z","shell.execute_reply":"2024-02-20T22:12:20.345714Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Context Function","metadata":{}},{"cell_type":"code","source":"def get_context(user_input_translated, db, k=1, fetch_k=200):\n    '''\n    Perform max marginal relevance search using the FAISS vector store\n    Search for relevant documents based on the transliterated user input\n    Usually you would want the fetch_k parameter >> k parameter\n    This is because the fetch_k parameter is the number of documents that will be fetched before filtering. \n    If you set fetch_k to a low number, you might not get enough documents to filter from.\n    k = Number of searches to be returned\n    fetch_k = Number of documents that will be fetched before filtering\n    '''\n    docs = db.max_marginal_relevance_search(user_input_translated, k=k, fetch_k=fetch_k) \n    # Initialize an empty string to store concatenated contexts\n    contexts = \"\"\n\n    # Iterate through the retrieved documents and print page content\n    for i, doc in enumerate(docs):\n        # print(f\"{i + 1}. {doc.page_content}\")\n\n        # Concatenate the page content for further processing if needed\n        contexts += doc.page_content + \"\\n\"\n    \n    return contexts","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:12:20.348011Z","iopub.execute_input":"2024-02-20T22:12:20.348401Z","iopub.status.idle":"2024-02-20T22:12:20.467176Z","shell.execute_reply.started":"2024-02-20T22:12:20.348354Z","shell.execute_reply":"2024-02-20T22:12:20.466147Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Making text look better","metadata":{}},{"cell_type":"code","source":"import pathlib\nimport textwrap\n\n# Import the generative AI module from the google package\nimport google.generativeai as genai\n\n# Import necessary display modules from IPython\nfrom IPython.display import display, Markdown\n\ndef to_markdown(text):\n    \"\"\"\n    Convert plain text to Markdown format.\n\n    This function takes a plain text input and converts it to Markdown format.\n    It also replaces bullet points with proper Markdown list syntax.\n\n    Args:\n        text (str): The plain text to be converted to Markdown.\n\n    Returns:\n        Markdown: The converted Markdown text.\n\n    Example:\n        >>> plain_text = \"This is a bullet point:\\n• Item 1\\n• Item 2\"\n        >>> markdown_output = to_markdown(plain_text)\n        >>> print(markdown_output)\n        > This is a bullet point:\n        >   * Item 1\n        >   * Item 2\n    \"\"\"\n    # Replace special character '•' with proper Markdown list syntax\n    text = text.replace('•', '  *')\n    \n    # Indent the text with '>' to format it as a blockquote\n    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:12:20.468178Z","iopub.execute_input":"2024-02-20T22:12:20.468487Z","iopub.status.idle":"2024-02-20T22:12:20.798341Z","shell.execute_reply.started":"2024-02-20T22:12:20.468461Z","shell.execute_reply":"2024-02-20T22:12:20.797619Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Search Function","metadata":{}},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:12:20.799378Z","iopub.execute_input":"2024-02-20T22:12:20.799666Z","iopub.status.idle":"2024-02-20T22:12:20.803613Z","shell.execute_reply.started":"2024-02-20T22:12:20.799641Z","shell.execute_reply":"2024-02-20T22:12:20.802677Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def search(query):\n    WebCrawler(query)\n    text_docs = docSplitter()\n    db = FAISS.from_documents(text_docs, embeddings)\n    relevant_info = get_context(query,db)\n    prompt = f\"\"\"<s>[INST] You are a helpful chatbot which uses the information provided to you help the users with their queries. Cite the websites used at the end.\n### Information: {relevant_info}\n\n### User Query: {query}\n[/INST]\"\"\"\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n    start = time.time()\n    sequences = pipe(\n            f'{prompt}' ,\n            do_sample=True,\n            max_new_tokens=2048, \n            temperature=0.7, \n            top_p=0.95\n        )\n    extracted_title = re.sub(r'[\\'\"]', '', sequences[0]['generated_text'].split(\"[/INST]\")[1])\n    stop = time.time()\n    time_taken = stop-start\n    print(f\"Execution Time : {time_taken}\")\n    return extracted_title\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:13:45.805504Z","iopub.execute_input":"2024-02-20T22:13:45.806219Z","iopub.status.idle":"2024-02-20T22:13:45.813378Z","shell.execute_reply.started":"2024-02-20T22:13:45.806185Z","shell.execute_reply":"2024-02-20T22:13:45.812418Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\nto_markdown(search(\"How do i install Linux on my external SSD step by step?\"))","metadata":{"execution":{"iopub.status.busy":"2024-02-20T22:13:46.645966Z","iopub.execute_input":"2024-02-20T22:13:46.646673Z","iopub.status.idle":"2024-02-20T22:14:48.039044Z","shell.execute_reply.started":"2024-02-20T22:13:46.646641Z","shell.execute_reply":"2024-02-20T22:14:48.038119Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"How do i install Linux on my external SSD step by step?How+do+i+install+Linux+on+my+external+SSD+step+by+step?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Time taken to execute:  1.9485552310943604\nExecution Time : 59.330870151519775\nCPU times: user 59.9 s, sys: 70.1 ms, total: 60 s\nWall time: 1min 1s\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":">  To install Linux on an external SSD, you can follow the steps below. These instructions are based on the article from Medium (<https://medium.com/geekculture/installing-linux-ubuntu-20-04-on-an-external-portable-ssd-and-pitfalls-to-be-aware-of-388294e701b5>) and some additional resources.\n> \n> 1. **Preparation:**\n>    a. Backup all your data on the external SSD.\n>    b. Install Ubuntu Desktop on a separate internal drive or a virtual machine if you dont have it already.\n>    c. Connect the SSD to your computer.\n> \n> 2. **Create a bootable USB:**\n>    a. Install `Rufus` or `Unetbootin` on your Ubuntu system.\n>    b. Use the software to create a bootable USB stick from the Ubuntu ISO file.\n> \n> 3. **Partition the SSD:**\n>    a. Open the Ubuntu installer using the bootable USB stick.\n>    b. In the Install Ubuntu window, select your language and click Continue.\n>    c. In the Disk usage screen, select the SSD and click Install Now.\n>    d. In the Partition Disks window, click New Partition Table.\n>    e. Create a new partition, mount point, and file system for your SSD. Be sure to allocate enough space for the root, home, swap, and boot partitions.\n>    f. Click Install Now and follow the on-screen instructions to complete the installation.\n> \n> 4. **Boot from the SSD:**\n>    a. Restart your computer and change the boot order in the BIOS settings to prioritize the SSD.\n>    b. If your computer doesnt boot from the SSD, you may need to modify the GRUB configuration file or use a bootloader like `rEFInd`.\n> \n> 5. **Install additional software:**\n>    a. Use the terminal to install additional software and drivers as needed.\n> \n> Some potential pitfalls to be aware of when installing Linux on an external SSD include:\n> \n> - External SSDs may not support hibernation or suspend-to-disk features.\n> - Depending on your hardware, you might face issues with power management and battery life when using the SSD as the main boot drive.\n> - Certain applications may not run optimally on an external SSD due to performance limitations.\n> \n> For more information and troubleshooting, refer to the article on Medium and the resources linked in this response.\n> \n> [Medium](https://medium.com/geekculture/installing-linux-ubuntu-20-04-on-an-external-portable-ssd-and-pitfalls-to-be-aware-of-388294e701b5), [Google Search: Featured snippets](https://support.google.com/websearch%3Fp%3Dfeatured_snippets%26hl%3Den-US&opi=89978449&usg=AOvVaw2M8SCMBzjmvsKhurArc7_8), [YouTube: Installing Ubuntu on an external SSD](https://www.youtube.com/watch?v=bfWdnCIrcxk), [YouTube: Installing Linux on an external SSD](https://www.youtube.com/watch?v=bfWdnCIrcxk)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}